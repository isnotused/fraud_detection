# é£é™©åˆ†æè¯„ä¼°
import numpy as np
import pandas as pd
from scipy.signal import correlate
import time
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px

data_title = ['æ¬ºè¯ˆæ£€æµ‹ç»“æœ', 'å¼‚å¸¸ç‰¹å¾åˆ†å¸ƒ', 'é£é™©å€¼è¶‹åŠ¿', 'äº¤æ˜“ç±»å‹åˆ†æ']


def calculate_time_offset(transaction_sequences):
    """ç¡®å®šä¸åŒäº¤æ˜“æ•°æ®åºåˆ—ä¹‹é—´çš„æ—¶åºåç§»é‡"""
    
    offsets = {}
    sequences = list(transaction_sequences.items())
    total_pairs = len(sequences) * (len(sequences) - 1) // 2
    pair_count = 0
    
    # é¢„è®¾åˆå§‹å€¼å’Œè¿­ä»£å¢é‡
    initial_offset = 0
    iteration_increment = 1
    
    for i in range(len(sequences)):
        seq1_id, seq1_data = sequences[i]
        for j in range(i + 1, len(sequences)):
            seq2_id, seq2_data = sequences[j]
            
            # ç¡®ä¿åºåˆ—é•¿åº¦ä¸€è‡´
            min_len = min(len(seq1_data), len(seq2_data))
            seq1 = seq1_data[:min_len]
            seq2 = seq2_data[:min_len]
            
            # åˆ©ç”¨æ•°å€¼å·®å¼‚æ„å»ºäº’ç›¸å…³å‡½æ•°
            diffs1 = np.diff(seq1)
            diffs2 = np.diff(seq2)
            
            # æ£€æŸ¥æ•°ç»„æ˜¯å¦ä¸ºç©ºï¼Œé¿å…correlateå‡½æ•°å‡ºé”™
            if len(diffs1) == 0 or len(diffs2) == 0:
                offsets[(seq1_id, seq2_id)] = 0
                pair_count += 1
                continue
            
            # è®¡ç®—äº’ç›¸å…³
            corr = correlate(diffs1, diffs2, mode='same')
            
            # æ‰¾åˆ°æœ€å¤§ç›¸å…³å€¼å¯¹åº”çš„åç§»é‡
            max_corr_idx = np.argmax(np.abs(corr))
            optimal_offset = max_corr_idx - len(corr) // 2
            
            # è¿­ä»£ä¼˜åŒ–åç§»é‡
            current_offset = initial_offset
            best_corr = -np.inf
            best_offset = current_offset
            
            # æœç´¢é™„è¿‘çš„åç§»é‡
            for offset in range(current_offset - 5, current_offset + 6, iteration_increment):
                if abs(offset) >= len(seq1) // 2:
                    continue
                
                if offset >= 0:
                    shifted_seq1 = seq1[offset:]
                    shifted_seq2 = seq2[:len(shifted_seq1)]
                else:
                    shifted_seq1 = seq1[:len(seq2) + offset]
                    shifted_seq2 = seq2[-offset:]
                
                if len(shifted_seq1) == 0 or len(shifted_seq2) == 0:
                    continue
                
                current_corr = np.corrcoef(shifted_seq1, shifted_seq2)[0, 1]
                
                if current_corr > best_corr:
                    best_corr = current_corr
                    best_offset = offset
            
            # ç»¼åˆä¸¤ç§æ–¹æ³•çš„ç»“æœ
            final_offset = best_offset if abs(best_corr) > 0.1 else optimal_offset
            
            offsets[(seq1_id, seq2_id)] = final_offset
            
            pair_count += 1
            progress = pair_count / total_pairs
            if pair_count % 5 == 0:
                time.sleep(0.1)
    
    return offsets

def build_user_behavior_graph(user_behavior_data, transaction_data):
    """æ„å»ºç”¨æˆ·è¡Œä¸ºäº¤äº’å›¾"""
    
    # è·å–æ‰€æœ‰ç”¨æˆ·
    users = user_behavior_data["user_id"].unique()
    user_index = {user: i for i, user in enumerate(users)}
    num_users = len(users)
    
    # åˆå§‹åŒ–è¿æ¥å¼ºåº¦çŸ©é˜µ
    connection_strength = np.zeros((num_users, num_users))
    
    # æŒ‰æ—¶é—´çª—å£åˆ†æç”¨æˆ·é—´çš„äº¤æ˜“è¿æ¥
    if "window_id" not in transaction_data.columns:
        # å¦‚æœæ²¡æœ‰window_idï¼Œè·³è¿‡çª—å£åˆ†æ
        return connection_strength, user_index
    
    windows = transaction_data["window_id"].unique()
    total_windows = len(windows)
    
    for i, window_id in enumerate(windows):
        window_trans = transaction_data[transaction_data["window_id"] == window_id]
        
        # å¯¹äºè½¬è´¦ç±»å‹ï¼Œå»ºç«‹ç”¨æˆ·é—´çš„è¿æ¥
        transfers = window_trans[window_trans["transaction_type"] == "è½¬è´¦"]
        
        for _, trans in transfers.iterrows():
            other_users = [u for u in users if u != trans["user_id"]]
            if other_users:
                recipient = np.random.choice(other_users)
                sender_idx = user_index[trans["user_id"]]
                recipient_idx = user_index[recipient]
                
                # è¿æ¥å¼ºåº¦ä¸äº¤æ˜“é‡‘é¢æ­£ç›¸å…³
                connection_strength[sender_idx, recipient_idx] += trans["amount"] / 1000  # å½’ä¸€åŒ–
                connection_strength[recipient_idx, sender_idx] += trans["amount"] / 1000  # åŒå‘è¿æ¥
        
        if i % 10 == 0:
            time.sleep(0.05)
    
    return connection_strength, user_index

def generate_behavior_feature_distribution(connection_strength, user_index, window_features, user_behavior_data):
    """ç”Ÿæˆç”¨æˆ·è¡Œä¸ºç‰¹å¾åˆ†å¸ƒæ›²çº¿"""

    # ä¸ºæ¯ä¸ªçª—å£ç”Ÿæˆè¡Œä¸ºç‰¹å¾åˆ†å¸ƒ
    windows = sorted(window_features["window_id"].unique())
    total_windows = len(windows)
    behavior_distributions = {}
    
    for i, window_id in enumerate(windows):
        # è®¡ç®—æ¯ä¸ªç”¨æˆ·çš„è¡Œä¸ºæ´»è·ƒåº¦
        user_activity = {}
        for user, idx in user_index.items():
            # è¿æ¥å¼ºåº¦æ€»å’Œè¡¨ç¤ºç”¨æˆ·çš„æ´»è·ƒåº¦
            activity = np.sum(connection_strength[idx, :])
            
            # ç»“åˆç”¨æˆ·è‡ªèº«ç‰¹å¾
            user_info = user_behavior_data[user_behavior_data["user_id"] == user].iloc[0]
            credit_factor = user_info["credit_score"] / 850  # ä¿¡ç”¨åˆ†æ•°å½’ä¸€åŒ–
            age_factor = 1 - abs(user_info["age"] - 35) / 50  
            
            # ç»¼åˆè¡Œä¸ºç‰¹å¾
            user_activity[user] = activity * credit_factor * age_factor
        
        # è½¬æ¢ä¸ºåˆ†å¸ƒ
        activity_values = np.array(list(user_activity.values()))
        if len(activity_values) > 0:
            activity_dist = activity_values / np.sum(activity_values)
            behavior_distributions[window_id] = activity_dist
        else:
            behavior_distributions[window_id] = np.array([])
        
        progress = (i + 1) / total_windows
        if i % 10 == 0:
            time.sleep(0.05)
    
    return behavior_distributions

def calculate_behavior_consistency(transaction_distributions, behavior_distributions, time_offsets):
    """è®¡ç®—æ¯ä¸ªæ—¶é—´çª—å£çš„è¡Œä¸ºä¸€è‡´æ€§æŒ‡æ ‡"""
    
    consistency_scores = {}
    window_ids = sorted(transaction_distributions.keys())
    total_windows = len(window_ids)
    
    # è®¡ç®—æ‰€æœ‰åç§»é‡çš„å¹³å‡å½±å“
    all_offsets = [abs(offset) for offset in time_offsets.values()]
    avg_offset = np.mean(all_offsets) if all_offsets else 0
    base_offset_impact = min(avg_offset / 100, 1.0)  # åŸºç¡€åç§»å½±å“
    
    for i, window_id in enumerate(window_ids):
        # è·å–å½“å‰çª—å£çš„äº¤æ˜“ç‰¹å¾åˆ†å¸ƒå’Œè¡Œä¸ºç‰¹å¾åˆ†å¸ƒ
        trans_dist = transaction_distributions.get(window_id, np.array([]))
        behav_dist = behavior_distributions.get(window_id, np.array([]))
        
        # ç¡®ä¿åˆ†å¸ƒé•¿åº¦ä¸€è‡´
        min_len = min(len(trans_dist), len(behav_dist))
        if min_len == 0:
            consistency_scores[window_id] = 0.0
            continue
        
        trans_dist = trans_dist[:min_len]
        behav_dist = behav_dist[:min_len]
        
        # è®¡ç®—åˆ†å¸ƒç›¸ä¼¼åº¦ - ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
        dot_product = np.dot(trans_dist, behav_dist)
        norm_trans = np.linalg.norm(trans_dist)
        norm_behav = np.linalg.norm(behav_dist)
        
        if norm_trans == 0 or norm_behav == 0:
            similarity = 0.0
        else:
            similarity = dot_product / (norm_trans * norm_behav)
        
        # ä½¿ç”¨åŸºç¡€åç§»å½±å“ï¼Œé¿å…ç±»å‹æ¯”è¾ƒé”™è¯¯
        offset_impact = base_offset_impact
        
        # ç»¼åˆè€ƒè™‘ç›¸ä¼¼åº¦å’Œåç§»å½±å“
        consistency = similarity * (1 - offset_impact)
        consistency_scores[window_id] = max(0, min(1, consistency))  # ç¡®ä¿åœ¨0-1ä¹‹é—´
        
        if i % 10 == 0:
            time.sleep(0.05)
    
    return consistency_scores

def generate_dynamic_risk_value(anomaly_features, transaction_features, behavior_distributions, consistency_scores):
    """æ ¹æ®æ‰€æœ‰æ—¶é—´çª—å£çš„å¼‚å¸¸ç‰¹å¾ç»´åº¦åˆ†å¸ƒç”ŸæˆåŠ¨æ€æ¬ºè¯ˆé£é™©å€¼"""
    progress = 0
    
    window_ids = sorted(transaction_features["window_id"].unique())
    total_windows = len(window_ids)
    risk_values = {}
    
    # æ”¶é›†æ‰€æœ‰çª—å£çš„å¼‚å¸¸ç‰¹å¾
    all_anomaly_features = [set(features) for features in anomaly_features.values()]
    
    # è®¡ç®—æ ¸å¿ƒå¼‚å¸¸ç»´åº¦ï¼ˆæ‰€æœ‰çª—å£å¼‚å¸¸ç‰¹å¾çš„äº¤é›†ï¼‰
    if all_anomaly_features:
        core_anomaly_dims = set.intersection(*[f for f in all_anomaly_features if f])
    else:
        core_anomaly_dims = set()
    
    # ä¸ºæ¯ä¸ªçª—å£è®¡ç®—é£é™©å€¼
    for i, window_id in enumerate(window_ids):
        # è·å–å½“å‰çª—å£çš„å¼‚å¸¸ç‰¹å¾
        window_anomalies = set(anomaly_features.get(window_id, []))
        
        # è®¡ç®—æ ¸å¿ƒå¼‚å¸¸ç»´åº¦æ•°å€¼åˆ†å¸ƒæ–¹å·®
        core_dims = list(core_anomaly_dims & window_anomalies)
        if core_dims:
            # è·å–æ ¸å¿ƒç»´åº¦çš„æ•°å€¼
            core_values = []
            for dim in core_dims:
                window_data = transaction_features[transaction_features["window_id"] == window_id]
                if not window_data.empty:
                    core_values.append(window_data.iloc[0][dim])
            
            if core_values:
                core_variance = np.var(core_values)
            else:
                core_variance = 0
        else:
            core_variance = 0
        
        # è®¡ç®—ç”¨æˆ·è¡Œä¸ºè½¨è¿¹å˜åŒ–é‡ï¼ˆä¸å‰ä¸€çª—å£æ¯”è¾ƒï¼‰
        if window_id > 0:
            behavior_change = calculate_user_behavior_change(behavior_distributions, window_id - 1, window_id)
        else:
            behavior_change = 0
        
        # è®¡ç®—æ¬ºè¯ˆé£é™©å› å­ï¼ˆåŠ æƒç»“æœï¼‰
        consistency_score = consistency_scores.get(window_id, 0)
        # ä¸€è‡´æ€§è¶Šä½ï¼Œé£é™©æƒé‡è¶Šé«˜
        consistency_weight = 1 - consistency_score
        
        # è®¡ç®—é£é™©å› å­
        risk_factor = (0.7 * core_variance * consistency_weight) + (0.3 * behavior_change)
        
        # å½’ä¸€åŒ–å¤„ç†
        max_possible_variance = 1e6  # é¢„è®¾æœ€å¤§å¯èƒ½æ–¹å·®
        normalized_risk = min(risk_factor / max_possible_variance, 1.0)
        
        # ç»“åˆå¼‚å¸¸ç‰¹å¾æ•°é‡è°ƒæ•´
        anomaly_count = len(window_anomalies)
        feature_count = len(transaction_features.columns) - 3  # æ’é™¤çª—å£IDå’Œæ—¶é—´åˆ—
        anomaly_ratio = anomaly_count / feature_count if feature_count > 0 else 0
        
        # æœ€ç»ˆé£é™©å€¼
        final_risk = min(normalized_risk + anomaly_ratio * 0.5, 1.0)
        risk_values[window_id] = final_risk
        
        if i % 10 == 0:
            time.sleep(0.05)
    
    return risk_values, core_anomaly_dims

def calculate_user_behavior_change(behavior_distributions, window_id1, window_id2):
    """è®¡ç®—ç”¨æˆ·è¡Œä¸ºè½¨è¿¹å˜åŒ–é‡"""
    dist1 = behavior_distributions.get(window_id1, np.array([]))
    dist2 = behavior_distributions.get(window_id2, np.array([]))
    
    # ç¡®ä¿åˆ†å¸ƒé•¿åº¦ä¸€è‡´
    min_len = min(len(dist1), len(dist2))
    if min_len == 0:
        return 0.0
    
    dist1 = dist1[:min_len]
    dist2 = dist2[:min_len]
    
    # è®¡ç®—JSæ•£åº¦ä½œä¸ºå˜åŒ–é‡
    eps = 1e-10
    avg_dist = (dist1 + dist2) / 2
    kl1 = np.sum(dist1 * np.log((dist1 + eps) / (avg_dist + eps)))
    kl2 = np.sum(dist2 * np.log((dist2 + eps) / (avg_dist + eps)))
    js_div = (kl1 + kl2) / 2
    return min(js_div, 1.0)  # é™åˆ¶æœ€å¤§å€¼å¹¶å½’ä¸€åŒ–

def tit_button(index):
    """æŒ‰é’®ç‚¹å‡»å›è°ƒå‡½æ•°"""
    st.session_state.analyzer_index = index
    st.session_state.analyzer_info = data_title[index]

def data_analyzer_app():
    st.title("ğŸ” é£é™©åˆ†æè¯„ä¼°")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®å¯ç”¨
    if 'user_data_generated' not in st.session_state or not st.session_state.user_data_generated:
        st.warning("âš ï¸ è¯·å…ˆåœ¨ã€ç”¨æˆ·æ•°æ®ã€‘é¡µé¢æŸ¥çœ‹æ•°æ®")
        return
    
    # åˆå§‹åŒ–session state
    if 'analyzer_index' not in st.session_state:
        st.session_state.analyzer_index = 0
        st.session_state.analyzer_info = data_title[0]
    
    # è‡ªåŠ¨æ‰§è¡Œé£é™©åˆ†æ
    if 'analysis_completed' not in st.session_state or not st.session_state.analysis_completed:
        with st.spinner("æ­£åœ¨è¿›è¡Œé£é™©åˆ†æ..."):
            perform_risk_analysis()
        st.session_state.analysis_completed = True
    
    tp = lambda x: 'primary' if st.session_state.analyzer_index == x else 'secondary'
    
    col01, col02 = st.columns([1, 5])
    
    with col01:
        st.markdown("### ğŸ“‹ åˆ†æè§†å›¾")
        with st.container(height=600, border=True):
            for ind, tit in enumerate(data_title):
                if st.button(label=tit, key=f'analyzer_tit_{ind}', use_container_width=True, 
                           on_click=tit_button, args=(ind,), type=tp(ind)):
                    pass
            
            st.divider()
            if st.button("ğŸ”„ åˆ·æ–°åˆ†æ", use_container_width=True):
                st.session_state.analysis_completed = False
                st.rerun()
    
    with col02:
        with st.container(border=True, height=600):
            if st.session_state.analyzer_index == 0:
                show_fraud_detection_results()
            elif st.session_state.analyzer_index == 1:
                show_anomaly_distribution()
            elif st.session_state.analyzer_index == 2:
                show_risk_trend()
            elif st.session_state.analyzer_index == 3:
                show_transaction_analysis()

def perform_risk_analysis():
    """æ‰§è¡Œé£é™©åˆ†æ"""
    # è·å–æ•°æ®
    transaction_data = st.session_state.transaction_data
    user_behavior_data = st.session_state.user_behavior_data
    segmented_data = st.session_state.segmented_data
    transaction_features = st.session_state.transaction_features
    
    # 1. æ„å»ºç”¨æˆ·è¡Œä¸ºå›¾
    connection_strength, user_index = build_user_behavior_graph(user_behavior_data, transaction_data)
    
    # 2. ç”Ÿæˆè¡Œä¸ºç‰¹å¾åˆ†å¸ƒ
    behavior_distributions = generate_behavior_feature_distribution(
        connection_strength, user_index, transaction_features, user_behavior_data
    )
    
    # 3. è®¡ç®—æ—¶é—´åç§»
    transaction_sequences = {}
    for window_id in transaction_features['window_id'].unique():
        window_data = segmented_data[segmented_data['window_id'] == window_id]
        transaction_sequences[window_id] = window_data['amount'].values
    
    time_offsets = calculate_time_offset(transaction_sequences)
    
    # 4. è®¡ç®—è¡Œä¸ºä¸€è‡´æ€§
    transaction_distributions = {wid: vals for wid, vals in transaction_sequences.items()}
    consistency_scores = calculate_behavior_consistency(
        transaction_distributions, behavior_distributions, time_offsets
    )
    
    # 5. æ£€æµ‹å¼‚å¸¸ç‰¹å¾
    anomaly_features = detect_anomaly_features(transaction_features)
    
    # 6. ç”ŸæˆåŠ¨æ€é£é™©å€¼
    dynamic_risk_values, core_anomaly_dims = generate_dynamic_risk_value(
        anomaly_features, transaction_features, behavior_distributions, consistency_scores
    )
    
    # 7. ç”Ÿæˆæ¬ºè¯ˆæ ‡è®°
    fraud_labels = generate_fraud_labels(dynamic_risk_values, transaction_features, segmented_data)
    
    # ä¿å­˜ç»“æœåˆ°session state
    st.session_state.consistency_scores = consistency_scores
    st.session_state.anomaly_features = anomaly_features
    st.session_state.dynamic_risk_values = dynamic_risk_values
    st.session_state.fraud_labels = fraud_labels

def detect_anomaly_features(transaction_features):
    """æ£€æµ‹å¼‚å¸¸ç‰¹å¾"""
    anomaly_features = {}
    
    for window_id in transaction_features['window_id'].unique():
        window_data = transaction_features[transaction_features['window_id'] == window_id].iloc[0]
        anomalies = []
        
        # æ£€æµ‹å¼‚å¸¸ç‰¹å¾
        if window_data['fraud_ratio'] > 0.05:  # æ¬ºè¯ˆç‡è¶…è¿‡5%
            anomalies.append('fraud_ratio')
        if window_data['avg_amount'] > 5000:  # å¹³å‡é‡‘é¢è¿‡å¤§
            anomalies.append('avg_amount')
        if window_data['max_amount'] > 50000:  # æœ€å¤§é‡‘é¢å¼‚å¸¸
            anomalies.append('max_amount')
        if window_data['total_transactions'] < 10:  # äº¤æ˜“æ•°é‡è¿‡å°‘
            anomalies.append('total_transactions')
            
        anomaly_features[window_id] = anomalies
    
    return anomaly_features

def generate_fraud_labels(dynamic_risk_values, transaction_features, segmented_data):
    """ç”Ÿæˆæ¬ºè¯ˆæ ‡è®°ç»“æœ - æŒ‰ç”¨æˆ·èšåˆ"""
    fraud_labels = []
    
    # æŒ‰ç”¨æˆ·èšåˆé£é™©æ•°æ®
    user_risk_data = {}
    
    for window_id in sorted(dynamic_risk_values.keys()):
        window_data = segmented_data[segmented_data['window_id'] == window_id]
        
        # éå†è¯¥çª—å£çš„æ‰€æœ‰ç”¨æˆ·
        for user_id in window_data['user_id'].unique():
            user_window_data = window_data[window_data['user_id'] == user_id]
            
            if user_id not in user_risk_data:
                user_risk_data[user_id] = {
                    'risk_values': [],
                    'transaction_count': 0,
                    'fraud_count': 0,
                    'windows': []
                }
            
            user_risk_data[user_id]['risk_values'].append(dynamic_risk_values[window_id])
            user_risk_data[user_id]['transaction_count'] += len(user_window_data)
            user_risk_data[user_id]['fraud_count'] += user_window_data['is_fraud'].sum()
            user_risk_data[user_id]['windows'].append(window_id)
    
    # ä¸ºæ¯ä¸ªç”¨æˆ·ç”Ÿæˆæ ‡è®°ç»“æœ
    for user_id, data in user_risk_data.items():
        # è®¡ç®—ç”¨æˆ·çš„å¹³å‡é£é™©å€¼
        avg_risk_value = np.mean(data['risk_values'])
        max_risk_value = np.max(data['risk_values'])
        
        # æ ¹æ®æœ€é«˜é£é™©å€¼åˆ¤å®šï¼ˆæ›´ä¸¥æ ¼ï¼‰
        if max_risk_value > 0.7:
            status = "æ¬ºè¯ˆ"
            color = "ğŸ”´"
        elif max_risk_value > 0.5:
            status = "æ¬ºè¯ˆ"
            color = "ğŸŸ¡"
        else:
            status = "æ­£å¸¸"
            color = "ğŸŸ¢"
        
        fraud_labels.append({
            'user_id': user_id,
            'risk_value': avg_risk_value,
            'max_risk_value': max_risk_value,
            'status': status,
            'color': color,
            'transaction_count': data['transaction_count'],
            'fraud_count': data['fraud_count'],
            'window_count': len(data['windows'])
        })
    
    return pd.DataFrame(fraud_labels).sort_values('max_risk_value', ascending=False)

def show_fraud_detection_results():
    """æ˜¾ç¤ºæ¬ºè¯ˆæ£€æµ‹ç»“æœ"""
    st.markdown("### ğŸ¯ æ¬ºè¯ˆæ£€æµ‹ç»“æœ")
    
    fraud_labels = st.session_state.fraud_labels
    
    # ç»Ÿè®¡æŒ‡æ ‡
    col1, col2, col3, col4 = st.columns(4)
    with col1:
        fraud_count = len(fraud_labels[fraud_labels['status'] == 'æ¬ºè¯ˆ'])
        st.metric("æ¬ºè¯ˆè¡Œä¸º", fraud_count, delta=f"{fraud_count/len(fraud_labels)*100:.1f}%", delta_color="inverse")
    with col2:
        normal_count = len(fraud_labels[fraud_labels['status'] == 'æ­£å¸¸'])
        st.metric("æ­£å¸¸è¡Œä¸º", normal_count, delta=f"{normal_count/len(fraud_labels)*100:.1f}%", delta_color="normal")
    with col3:
        total_windows = len(fraud_labels)
        st.metric("æ€»æ—¶é—´çª—å£", total_windows)
    with col4:
        total_fraud = fraud_labels['fraud_count'].sum()
        st.metric("å¼‚å¸¸äº¤æ˜“æ•°", f"{total_fraud}")
    
    # æ˜¾ç¤ºæ£€æµ‹ç»“æœè¡¨æ ¼
    st.markdown("#### ç”¨æˆ·è¡Œä¸ºæ ‡è®°ç»“æœ")
    display_df = fraud_labels.copy()
    display_df['risk_value'] = display_df['risk_value'].apply(lambda x: f"{x:.3f}")
    display_df['max_risk_value'] = display_df['max_risk_value'].apply(lambda x: f"{x:.3f}")
    display_df = display_df[['color', 'user_id', 'status', 'max_risk_value', 'risk_value', 'transaction_count', 'fraud_count', 'window_count']]
    display_df.columns = ['çŠ¶æ€', 'ç”¨æˆ·ID', 'è¡Œä¸ºæ ‡è®°', 'æœ€é«˜é£é™©å€¼', 'å¹³å‡é£é™©å€¼', 'äº¤æ˜“æ•°', 'å¼‚å¸¸æ•°', 'çª—å£æ•°']
    st.dataframe(display_df, use_container_width=True, height=400)

def show_anomaly_distribution():
    """æ˜¾ç¤ºå¼‚å¸¸ç‰¹å¾åˆ†å¸ƒ"""
    st.markdown("### ğŸ“Š å„æ—¶é—´çª—å£å¼‚å¸¸ç‰¹å¾åˆ†å¸ƒ")
    
    anomaly_features = st.session_state.anomaly_features
    
    # åˆ›å»ºçƒ­åŠ›å›¾
    fig = create_anomaly_heatmap(anomaly_features)
    st.plotly_chart(fig, use_container_width=True)

def create_anomaly_heatmap(anomaly_features):
    """åˆ›å»ºå¼‚å¸¸ç‰¹å¾çƒ­åŠ›å›¾"""
    window_ids = sorted(anomaly_features.keys())
    all_features = set()
    for features in anomaly_features.values():
        all_features.update(features)
    all_features = sorted(list(all_features))
    
    if not all_features:
        all_features = ['æ— å¼‚å¸¸']
    
    # åˆ›å»ºç‰¹å¾çŸ©é˜µ
    matrix = []
    for window_id in window_ids:
        row = [1 if f in anomaly_features.get(window_id, []) else 0 for f in all_features]
        matrix.append(row)
    
    fig = go.Figure(data=go.Heatmap(
        z=matrix,
        x=all_features,
        y=window_ids,
        colorscale='Reds',
        showscale=True,
        colorbar=dict(title='å¼‚å¸¸çŠ¶æ€')
    ))
    
    fig.update_layout(
        title='å¼‚å¸¸ç‰¹å¾åˆ†å¸ƒçƒ­åŠ›å›¾',
        xaxis_title='ç‰¹å¾ç»´åº¦',
        yaxis_title='æ—¶é—´çª—å£ID',
        plot_bgcolor='rgba(240,240,240,0.5)',
        height=500
    )
    
    return fig

def show_risk_trend():
    """æ˜¾ç¤ºé£é™©å€¼è¶‹åŠ¿"""
    st.markdown("### ğŸ“ˆ æ—¶é—´çª—å£åŠ¨æ€æ¬ºè¯ˆé£é™©å€¼è¶‹åŠ¿")
    
    dynamic_risk_values = st.session_state.dynamic_risk_values
    
    fig = create_risk_trend_chart(dynamic_risk_values)
    st.plotly_chart(fig, use_container_width=True)

def create_risk_trend_chart(risk_values):
    """åˆ›å»ºé£é™©è¶‹åŠ¿å›¾"""
    window_ids = sorted(risk_values.keys())
    risks = [risk_values[wid] for wid in window_ids]
    
    fig = go.Figure()
    
    # æ·»åŠ é£é™©æ›²çº¿
    fig.add_trace(go.Scatter(
        x=window_ids,
        y=risks,
        mode='lines+markers',
        name='é£é™©å€¼',
        line=dict(color='#E74C3C', width=3),
        marker=dict(size=8, symbol='circle'),
        fill='tozeroy',
        fillcolor='rgba(231, 76, 60, 0.2)',
        hovertemplate='çª—å£ %{x}<br>é£é™©å€¼: %{y:.3f}<extra></extra>'
    ))
    
    # æ·»åŠ é£é™©é˜ˆå€¼çº¿
    fig.add_hline(y=0.7, line_dash="dash", line_color="red", 
                  annotation_text="é«˜é£é™©é˜ˆå€¼", annotation_position="right")
    fig.add_hline(y=0.5, line_dash="dot", line_color="orange", 
                  annotation_text="ä¸­é£é™©é˜ˆå€¼", annotation_position="right")
    
    fig.update_layout(
        title='åŠ¨æ€æ¬ºè¯ˆé£é™©å€¼è¶‹åŠ¿',
        xaxis_title='æ—¶é—´çª—å£ID',
        yaxis_title='é£é™©å€¼',
        plot_bgcolor='rgba(240,240,240,0.5)',
        yaxis=dict(range=[0, 1.1]),
        height=450,
        hovermode='x unified'
    )
    
    return fig

def show_transaction_analysis():
    """æ˜¾ç¤ºäº¤æ˜“ç±»å‹åˆ†æ"""
    st.markdown("### ğŸ’° äº¤æ˜“ç±»å‹åˆ†å¸ƒä¸æ¬ºè¯ˆå…³è”å›¾")
    
    transaction_data = st.session_state.transaction_data
    
    fig = create_transaction_fraud_chart(transaction_data)
    st.plotly_chart(fig, use_container_width=True)

def create_transaction_fraud_chart(transaction_data):
    """åˆ›å»ºäº¤æ˜“ç±»å‹æ¬ºè¯ˆå…³è”å›¾"""
    # æŒ‰äº¤æ˜“ç±»å‹ç»Ÿè®¡
    trans_stats = transaction_data.groupby('transaction_type').agg({
        'is_fraud': ['sum', 'mean', 'count']
    }).reset_index()
    trans_stats.columns = ['transaction_type', 'fraud_count', 'fraud_rate', 'total_count']
    trans_stats = trans_stats.sort_values('fraud_rate', ascending=False)
    
    # åˆ›å»ºåŒè½´å›¾è¡¨
    fig = go.Figure()
    
    # æŸ±çŠ¶å›¾ï¼šäº¤æ˜“æ•°é‡
    fig.add_trace(go.Bar(
        x=trans_stats['transaction_type'],
        y=trans_stats['total_count'],
        name='äº¤æ˜“æ•°é‡',
        marker=dict(color='lightblue'),
        yaxis='y',
        hovertemplate='%{x}<br>äº¤æ˜“æ•°: %{y}<extra></extra>'
    ))
    
    # æŠ˜çº¿å›¾ï¼šæ¬ºè¯ˆç‡
    fig.add_trace(go.Scatter(
        x=trans_stats['transaction_type'],
        y=trans_stats['fraud_rate'] * 100,
        name='æ¬ºè¯ˆç‡ (%)',
        mode='lines+markers',
        line=dict(color='red', width=3),
        marker=dict(size=10, symbol='diamond'),
        yaxis='y2',
        hovertemplate='%{x}<br>æ¬ºè¯ˆç‡: %{y:.2f}%<extra></extra>'
    ))
    
    fig.update_layout(
        title='äº¤æ˜“ç±»å‹åˆ†å¸ƒä¸æ¬ºè¯ˆå…³è”åˆ†æ',
        xaxis_title='äº¤æ˜“ç±»å‹',
        yaxis=dict(title='äº¤æ˜“æ•°é‡', side='left'),
        yaxis2=dict(title='æ¬ºè¯ˆç‡ (%)', side='right', overlaying='y'),
        plot_bgcolor='rgba(240,240,240,0.5)',
        height=450,
        hovermode='x unified',
        legend=dict(x=0.01, y=0.99)
    )
    
    return fig

